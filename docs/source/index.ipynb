{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![EvalML Logo](images/evalml_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is EvalML\n",
    "EvalML is an AutoML library to build optimized machine learning pipelines for domain-specific objective functions.\n",
    "\n",
    "\n",
    "Combined with `Featuretools <www.featuretools.com>`__ and `Compose <compose.ml>`__, EvalML can be used to create a state-of-the-art end-to-to end machine learning solutions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import evalml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "First, we load in the features and targets we want to use to train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = evalml.demos.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure search\n",
    "\n",
    "EvalML has many options to configure the pipeline search. At the minimum, we need to define an objective function. For simplicity, we will use the F1 score in this example as the objective function. However, the real power of EvalML is in using [domain-specific]() objective functions or [building your own](). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = evalml.AutoClassifier(objective=\"f1\",\n",
    "                            max_pipelines=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to validate the results of the pipeline creation and optimization process, we will save some of our data as a holdout set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_holdout, y_train, y_holdout = evalml.preprocessing.split_data(X, y, test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we call `.fit()`, the search for the best pipeline will begin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m*****************************\u001b[0m\n",
      "\u001b[1m* Beginning pipeline search *\u001b[0m\n",
      "\u001b[1m*****************************\u001b[0m\n",
      "\n",
      "Optimizing for F1. Greater score is better.\n",
      "\n",
      "Searching up to 5 pipelines. No time limit is set. Set one using max_time parameter.\n",
      "\n",
      "Possible model types: random_forest, xgboost, linear_model\n",
      "\n",
      "Testing Random Forest w/ imputation:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See Pipeline Rankings\n",
    "\n",
    "After the search is finished we can view all of the pipelines searched, ranked by score. Internally, EvalML performs cross validation to score the pipelines. If it notices, a high variance across cross validation folds, it will warn you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe pipeline\n",
    "\n",
    "If we are interested in see more details about the pipeline we can describe it using the `id` from the rankings table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.describe_pipeline(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Best pipeline\n",
    "We can now select best pipeline and score it on our holdout data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = clf.best_pipeline\n",
    "pipeline.score(X_holdout, y_holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbsphinx-toctree": {
     "maxdepth": 1
    }
   },
   "source": [
    "# Getting Started\n",
    "[What is EvalML](self)\n",
    "\n",
    "[Install](install)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbsphinx-toctree": {
     "maxdepth": 1
    }
   },
   "source": [
    "# Overview\n",
    "[Pipeline Search](overview/pipeline_search)\n",
    "\n",
    "[Objective  Functions](overview/objective_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbsphinx-toctree": {
     "maxdepth": 1
    }
   },
   "source": [
    "# Demos\n",
    "\n",
    "[Fraud Prediction](demos/fraud)\n",
    "\n",
    "[Lead Scoring](demos/lead_scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbsphinx-toctree": {
     "maxdepth": 1
    }
   },
   "source": [
    "# Resources\n",
    "[API Reference](api_reference)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
