{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    }
   ],
   "source": [
    "from evalml.demos import load_fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import LocalCluster, Client\n",
    "\n",
    "from evalml.automl import AutoMLSearch\n",
    "from evalml.automl.engine import SequentialEngine, DaskEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Number of Features\n",
      "Boolean                       1\n",
      "Categorical                   6\n",
      "Numeric                       5\n",
      "\n",
      "Number of training examples: 20000\n",
      "Targets\n",
      "False    84.97%\n",
      "True     15.02%\n",
      "Name: fraud, dtype: object\n"
     ]
    }
   ],
   "source": [
    "X, y = load_fraud(n_rows=20000, return_pandas=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_engine(engine_cfg):\n",
    "    if engine_cfg[\"type\"] == \"sequential\":\n",
    "        engine = SequentialEngine\n",
    "        return engine()\n",
    "    elif engine_cfg[\"type\"] == \"dask\":\n",
    "        engine = DaskEngine\n",
    "        cluster = LocalCluster(engine_cfg[\"n_workers\"])\n",
    "        client = Client(cluster)\n",
    "        return engine(client)\n",
    "    \n",
    "engine_cfgs = [{\"type\": \"sequential\", \"n_workers\": 1}, \n",
    "              {\"type\": \"dask\", \"n_workers\": 1}, \n",
    "              {\"type\": \"dask\", \"n_workers\": 2}, \n",
    "              {\"type\": \"dask\", \"n_workers\": 4}, \n",
    "              {\"type\": \"dask\", \"n_workers\": 8}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Number of Features\n",
      "Boolean                       1\n",
      "Categorical                   6\n",
      "Numeric                       5\n",
      "\n",
      "Number of training examples: 128\n",
      "Targets\n",
      "False    89.06%\n",
      "True     10.94%\n",
      "Name: fraud, dtype: object\n",
      "Using default limit of max_batches=1.\n",
      "\n",
      "Generating pipelines to search over...\n",
      "\n",
      "*****************************\n",
      "* Beginning pipeline search *\n",
      "*****************************\n",
      "\n",
      "Optimizing for Log Loss Binary. \n",
      "Lower score is better.\n",
      "\n",
      "Using SequentialEngine to train and score pipelines.\n",
      "Searching up to 1 batches for a total of 9 pipelines. \n",
      "Allowed model families: xgboost, random_forest, extra_trees, decision_tree, lightgbm, linear_model, catboost\n",
      "\n",
      "Evaluating Baseline Pipeline: Mode Baseline Binary Classification Pipeline\n",
      "Mode Baseline Binary Classification Pipeline:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 3.774\n",
      "\n",
      "*****************************\n",
      "* Evaluating Batch Number 1 *\n",
      "*****************************\n",
      "\n",
      "Decision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 3.994\n",
      "\tHigh coefficient of variation (cv >= 0.2) within cross validation scores.\n",
      "\tDecision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder may not perform as estimated on unseen data.\n",
      "Extra Trees Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.365\n",
      "CatBoost Classifier w/ Imputer + DateTime Featurization Component:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.594\n",
      "Random Forest Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.274\n",
      "LightGBM Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.339\n",
      "XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.237\n",
      "Elastic Net Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.347\n",
      "Logistic Regression Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.700\n",
      "\n",
      "Search finished after 00:19            \n",
      "Best pipeline: XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder\n",
      "Best pipeline Log Loss Binary: 0.236720\n",
      "19.818434715270996\n",
      "             Number of Features\n",
      "Boolean                       1\n",
      "Categorical                   6\n",
      "Numeric                       5\n",
      "\n",
      "Number of training examples: 2048\n",
      "Targets\n",
      "False    86.23%\n",
      "True     13.77%\n",
      "Name: fraud, dtype: object\n",
      "Using default limit of max_batches=1.\n",
      "\n",
      "Generating pipelines to search over...\n",
      "\n",
      "*****************************\n",
      "* Beginning pipeline search *\n",
      "*****************************\n",
      "\n",
      "Optimizing for Log Loss Binary. \n",
      "Lower score is better.\n",
      "\n",
      "Using SequentialEngine to train and score pipelines.\n",
      "Searching up to 1 batches for a total of 9 pipelines. \n",
      "Allowed model families: xgboost, random_forest, extra_trees, decision_tree, lightgbm, linear_model, catboost\n",
      "\n",
      "Evaluating Baseline Pipeline: Mode Baseline Binary Classification Pipeline\n",
      "Mode Baseline Binary Classification Pipeline:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 4.756\n",
      "\n",
      "*****************************\n",
      "* Evaluating Batch Number 1 *\n",
      "*****************************\n",
      "\n",
      "Decision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.637\n",
      "Extra Trees Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.365\n",
      "CatBoost Classifier w/ Imputer + DateTime Featurization Component:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.535\n",
      "Random Forest Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.261\n",
      "LightGBM Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.286\n",
      "XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.213\n",
      "Elastic Net Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.416\n",
      "Logistic Regression Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.360\n",
      "\n",
      "Search finished after 00:19            \n",
      "Best pipeline: XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder\n",
      "Best pipeline Log Loss Binary: 0.213329\n",
      "19.826002836227417\n",
      "             Number of Features\n",
      "Boolean                       1\n",
      "Categorical                   6\n",
      "Numeric                       5\n",
      "\n",
      "Number of training examples: 32768\n",
      "Targets\n",
      "False    84.90%\n",
      "True     15.10%\n",
      "Name: fraud, dtype: object\n",
      "Using default limit of max_batches=1.\n",
      "\n",
      "Generating pipelines to search over...\n",
      "\n",
      "*****************************\n",
      "* Beginning pipeline search *\n",
      "*****************************\n",
      "\n",
      "Optimizing for Log Loss Binary. \n",
      "Lower score is better.\n",
      "\n",
      "Using SequentialEngine to train and score pipelines.\n",
      "Searching up to 1 batches for a total of 9 pipelines. \n",
      "Allowed model families: xgboost, random_forest, extra_trees, decision_tree, lightgbm, linear_model, catboost\n",
      "\n",
      "Evaluating Baseline Pipeline: Mode Baseline Binary Classification Pipeline\n",
      "Mode Baseline Binary Classification Pipeline:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 5.215\n",
      "\n",
      "*****************************\n",
      "* Evaluating Batch Number 1 *\n",
      "*****************************\n",
      "\n",
      "Decision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.390\n",
      "Extra Trees Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.373\n",
      "CatBoost Classifier w/ Imputer + DateTime Featurization Component:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.516\n",
      "Random Forest Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.268\n",
      "LightGBM Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.191\n",
      "XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.188\n",
      "Elastic Net Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.432\n",
      "Logistic Regression Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.331\n",
      "\n",
      "Search finished after 00:38            \n",
      "Best pipeline: XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder\n",
      "Best pipeline Log Loss Binary: 0.188460\n",
      "38.102290868759155\n",
      "             Number of Features\n",
      "Boolean                       1\n",
      "Categorical                   6\n",
      "Numeric                       5\n",
      "\n",
      "Number of training examples: 99992\n",
      "Targets\n",
      "False    84.82%\n",
      "True     15.18%\n",
      "Name: fraud, dtype: object\n",
      "Using default limit of max_batches=1.\n",
      "\n",
      "Generating pipelines to search over...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*****************************\n",
      "* Beginning pipeline search *\n",
      "*****************************\n",
      "\n",
      "Optimizing for Log Loss Binary. \n",
      "Lower score is better.\n",
      "\n",
      "Using SequentialEngine to train and score pipelines.\n",
      "Searching up to 1 batches for a total of 9 pipelines. \n",
      "Allowed model families: xgboost, random_forest, extra_trees, decision_tree, lightgbm, linear_model, catboost\n",
      "\n",
      "Evaluating Baseline Pipeline: Mode Baseline Binary Classification Pipeline\n",
      "Mode Baseline Binary Classification Pipeline:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 5.243\n",
      "\n",
      "*****************************\n",
      "* Evaluating Batch Number 1 *\n",
      "*****************************\n",
      "\n",
      "Decision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.288\n",
      "\tHigh coefficient of variation (cv >= 0.2) within cross validation scores.\n",
      "\tDecision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder may not perform as estimated on unseen data.\n",
      "Extra Trees Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.372\n",
      "CatBoost Classifier w/ Imputer + DateTime Featurization Component:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.514\n",
      "Random Forest Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.258\n",
      "LightGBM Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.193\n",
      "XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.192\n",
      "Elastic Net Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.435\n",
      "Logistic Regression Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.332\n",
      "\n",
      "Search finished after 01:14            \n",
      "Best pipeline: XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder\n",
      "Best pipeline Log Loss Binary: 0.191739\n",
      "74.45213007926941\n",
      "             Number of Features\n",
      "Boolean                       1\n",
      "Categorical                   6\n",
      "Numeric                       5\n",
      "\n",
      "Number of training examples: 99992\n",
      "Targets\n",
      "False    84.82%\n",
      "True     15.18%\n",
      "Name: fraud, dtype: object\n",
      "Using default limit of max_batches=1.\n",
      "\n",
      "Generating pipelines to search over...\n",
      "\n",
      "*****************************\n",
      "* Beginning pipeline search *\n",
      "*****************************\n",
      "\n",
      "Optimizing for Log Loss Binary. \n",
      "Lower score is better.\n",
      "\n",
      "Using SequentialEngine to train and score pipelines.\n",
      "Searching up to 1 batches for a total of 9 pipelines. \n",
      "Allowed model families: xgboost, random_forest, extra_trees, decision_tree, lightgbm, linear_model, catboost\n",
      "\n",
      "Evaluating Baseline Pipeline: Mode Baseline Binary Classification Pipeline\n",
      "Mode Baseline Binary Classification Pipeline:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 5.243\n",
      "\n",
      "*****************************\n",
      "* Evaluating Batch Number 1 *\n",
      "*****************************\n",
      "\n",
      "Decision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.288\n",
      "\tHigh coefficient of variation (cv >= 0.2) within cross validation scores.\n",
      "\tDecision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder may not perform as estimated on unseen data.\n",
      "Extra Trees Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.372\n",
      "CatBoost Classifier w/ Imputer + DateTime Featurization Component:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.514\n",
      "Random Forest Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.258\n",
      "LightGBM Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.193\n",
      "XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.192\n",
      "Elastic Net Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.435\n",
      "Logistic Regression Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.332\n",
      "\n",
      "Search finished after 01:20            \n",
      "Best pipeline: XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder\n",
      "Best pipeline Log Loss Binary: 0.191739\n",
      "80.4342348575592\n",
      "             Number of Features\n",
      "Boolean                       1\n",
      "Categorical                   6\n",
      "Numeric                       5\n",
      "\n",
      "Number of training examples: 128\n",
      "Targets\n",
      "False    89.06%\n",
      "True     10.94%\n",
      "Name: fraud, dtype: object\n",
      "Using default limit of max_batches=1.\n",
      "\n",
      "Generating pipelines to search over...\n",
      "\n",
      "*****************************\n",
      "* Beginning pipeline search *\n",
      "*****************************\n",
      "\n",
      "Optimizing for Log Loss Binary. \n",
      "Lower score is better.\n",
      "\n",
      "Using DaskEngine to train and score pipelines.\n",
      "Searching up to 1 batches for a total of 9 pipelines. \n",
      "Allowed model families: xgboost, random_forest, extra_trees, decision_tree, lightgbm, linear_model, catboost\n",
      "\n",
      "Evaluating Baseline Pipeline: Mode Baseline Binary Classification Pipeline\n",
      "Mode Baseline Binary Classification Pipeline:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 3.774\n",
      "\n",
      "*****************************\n",
      "* Evaluating Batch Number 1 *\n",
      "*****************************\n",
      "\n",
      "Decision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 3.994\n",
      "\tHigh coefficient of variation (cv >= 0.2) within cross validation scores.\n",
      "\tDecision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder may not perform as estimated on unseen data.\n",
      "CatBoost Classifier w/ Imputer + DateTime Featurization Component:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.594\n",
      "LightGBM Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.339\n",
      "XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.237\n",
      "Elastic Net Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.347\n",
      "Extra Trees Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.365\n",
      "Logistic Regression Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.700\n",
      "Random Forest Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.274\n",
      "\n",
      "Search finished after 00:17            \n",
      "Best pipeline: XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder\n",
      "Best pipeline Log Loss Binary: 0.236720\n",
      "17.34480905532837\n",
      "             Number of Features\n",
      "Boolean                       1\n",
      "Categorical                   6\n",
      "Numeric                       5\n",
      "\n",
      "Number of training examples: 2048\n",
      "Targets\n",
      "False    86.23%\n",
      "True     13.77%\n",
      "Name: fraud, dtype: object\n",
      "Using default limit of max_batches=1.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating pipelines to search over...\n",
      "\n",
      "*****************************\n",
      "* Beginning pipeline search *\n",
      "*****************************\n",
      "\n",
      "Optimizing for Log Loss Binary. \n",
      "Lower score is better.\n",
      "\n",
      "Using DaskEngine to train and score pipelines.\n",
      "Searching up to 1 batches for a total of 9 pipelines. \n",
      "Allowed model families: xgboost, random_forest, extra_trees, decision_tree, lightgbm, linear_model, catboost\n",
      "\n",
      "Evaluating Baseline Pipeline: Mode Baseline Binary Classification Pipeline\n",
      "Mode Baseline Binary Classification Pipeline:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 4.756\n",
      "\n",
      "*****************************\n",
      "* Evaluating Batch Number 1 *\n",
      "*****************************\n",
      "\n",
      "CatBoost Classifier w/ Imputer + DateTime Featurization Component:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.534\n",
      "Decision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 1.032\n",
      "\tHigh coefficient of variation (cv >= 0.2) within cross validation scores.\n",
      "\tDecision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder may not perform as estimated on unseen data.\n",
      "XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.210\n",
      "LightGBM Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.283\n",
      "Elastic Net Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.416\n",
      "Extra Trees Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.363\n",
      "Logistic Regression Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.368\n",
      "Random Forest Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.267\n",
      "\n",
      "Search finished after 00:10            \n",
      "Best pipeline: XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder\n",
      "Best pipeline Log Loss Binary: 0.209978\n",
      "10.225600957870483\n",
      "             Number of Features\n",
      "Boolean                       1\n",
      "Categorical                   6\n",
      "Numeric                       5\n",
      "\n",
      "Number of training examples: 32768\n",
      "Targets\n",
      "False    84.90%\n",
      "True     15.10%\n",
      "Name: fraud, dtype: object\n",
      "Using default limit of max_batches=1.\n",
      "\n",
      "Generating pipelines to search over...\n",
      "\n",
      "*****************************\n",
      "* Beginning pipeline search *\n",
      "*****************************\n",
      "\n",
      "Optimizing for Log Loss Binary. \n",
      "Lower score is better.\n",
      "\n",
      "Using DaskEngine to train and score pipelines.\n",
      "Searching up to 1 batches for a total of 9 pipelines. \n",
      "Allowed model families: xgboost, random_forest, extra_trees, decision_tree, lightgbm, linear_model, catboost\n",
      "\n",
      "Evaluating Baseline Pipeline: Mode Baseline Binary Classification Pipeline\n",
      "Mode Baseline Binary Classification Pipeline:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 5.215\n",
      "\n",
      "*****************************\n",
      "* Evaluating Batch Number 1 *\n",
      "*****************************\n",
      "\n",
      "CatBoost Classifier w/ Imputer + DateTime Featurization Component:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.516\n",
      "Decision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.377\n",
      "\tHigh coefficient of variation (cv >= 0.2) within cross validation scores.\n",
      "\tDecision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder may not perform as estimated on unseen data.\n",
      "Random Forest Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.267\n",
      "Extra Trees Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.370\n",
      "XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.189\n",
      "LightGBM Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.190\n",
      "Elastic Net Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.433\n",
      "Logistic Regression Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.330\n",
      "\n",
      "Search finished after 00:22            \n",
      "Best pipeline: XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder\n",
      "Best pipeline Log Loss Binary: 0.188965\n",
      "22.06515097618103\n",
      "             Number of Features\n",
      "Boolean                       1\n",
      "Categorical                   6\n",
      "Numeric                       5\n",
      "\n",
      "Number of training examples: 99992\n",
      "Targets\n",
      "False    84.82%\n",
      "True     15.18%\n",
      "Name: fraud, dtype: object\n",
      "Using default limit of max_batches=1.\n",
      "\n",
      "Generating pipelines to search over...\n",
      "\n",
      "*****************************\n",
      "* Beginning pipeline search *\n",
      "*****************************\n",
      "\n",
      "Optimizing for Log Loss Binary. \n",
      "Lower score is better.\n",
      "\n",
      "Using DaskEngine to train and score pipelines.\n",
      "Searching up to 1 batches for a total of 9 pipelines. \n",
      "Allowed model families: xgboost, random_forest, extra_trees, decision_tree, lightgbm, linear_model, catboost\n",
      "\n",
      "Evaluating Baseline Pipeline: Mode Baseline Binary Classification Pipeline\n",
      "Mode Baseline Binary Classification Pipeline:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 5.243\n",
      "\n",
      "*****************************\n",
      "* Evaluating Batch Number 1 *\n",
      "*****************************\n",
      "\n",
      "CatBoost Classifier w/ Imputer + DateTime Featurization Component:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.514\n",
      "Decision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.435\n",
      "Random Forest Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.262\n",
      "Extra Trees Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.373\n",
      "Logistic Regression Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.332\n",
      "Elastic Net Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.434\n",
      "XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.192\n",
      "LightGBM Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.193\n",
      "\n",
      "Search finished after 00:40            \n",
      "Best pipeline: XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder\n",
      "Best pipeline Log Loss Binary: 0.191789\n",
      "40.02791094779968\n",
      "             Number of Features\n",
      "Boolean                       1\n",
      "Categorical                   6\n",
      "Numeric                       5\n",
      "\n",
      "Number of training examples: 99992\n",
      "Targets\n",
      "False    84.82%\n",
      "True     15.18%\n",
      "Name: fraud, dtype: object\n",
      "Using default limit of max_batches=1.\n",
      "\n",
      "Generating pipelines to search over...\n",
      "\n",
      "*****************************\n",
      "* Beginning pipeline search *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************\n",
      "\n",
      "Optimizing for Log Loss Binary. \n",
      "Lower score is better.\n",
      "\n",
      "Using DaskEngine to train and score pipelines.\n",
      "Searching up to 1 batches for a total of 9 pipelines. \n",
      "Allowed model families: xgboost, random_forest, extra_trees, decision_tree, lightgbm, linear_model, catboost\n",
      "\n",
      "Evaluating Baseline Pipeline: Mode Baseline Binary Classification Pipeline\n",
      "Mode Baseline Binary Classification Pipeline:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 5.243\n",
      "\n",
      "*****************************\n",
      "* Evaluating Batch Number 1 *\n",
      "*****************************\n",
      "\n",
      "CatBoost Classifier w/ Imputer + DateTime Featurization Component:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.514\n",
      "Decision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.435\n",
      "Extra Trees Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.373\n",
      "Random Forest Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.262\n",
      "Logistic Regression Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.332\n",
      "Elastic Net Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.434\n",
      "XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.192\n",
      "LightGBM Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.193\n",
      "\n",
      "Search finished after 00:32            \n",
      "Best pipeline: XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder\n",
      "Best pipeline Log Loss Binary: 0.191789\n",
      "32.725412130355835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/evalml_dev/lib/python3.9/site-packages/distributed/node.py:151: UserWarning:\n",
      "\n",
      "Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 51143 instead\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Number of Features\n",
      "Boolean                       1\n",
      "Categorical                   6\n",
      "Numeric                       5\n",
      "\n",
      "Number of training examples: 128\n",
      "Targets\n",
      "False    89.06%\n",
      "True     10.94%\n",
      "Name: fraud, dtype: object\n",
      "Using default limit of max_batches=1.\n",
      "\n",
      "Generating pipelines to search over...\n",
      "\n",
      "*****************************\n",
      "* Beginning pipeline search *\n",
      "*****************************\n",
      "\n",
      "Optimizing for Log Loss Binary. \n",
      "Lower score is better.\n",
      "\n",
      "Using DaskEngine to train and score pipelines.\n",
      "Searching up to 1 batches for a total of 9 pipelines. \n",
      "Allowed model families: xgboost, random_forest, extra_trees, decision_tree, lightgbm, linear_model, catboost\n",
      "\n",
      "Evaluating Baseline Pipeline: Mode Baseline Binary Classification Pipeline\n",
      "Mode Baseline Binary Classification Pipeline:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 3.774\n",
      "\n",
      "*****************************\n",
      "* Evaluating Batch Number 1 *\n",
      "*****************************\n",
      "\n",
      "Decision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 3.994\n",
      "\tHigh coefficient of variation (cv >= 0.2) within cross validation scores.\n",
      "\tDecision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder may not perform as estimated on unseen data.\n",
      "CatBoost Classifier w/ Imputer + DateTime Featurization Component:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.594\n",
      "LightGBM Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.339\n",
      "XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.237\n",
      "Random Forest Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.274\n",
      "Extra Trees Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.365\n",
      "Logistic Regression Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.700\n",
      "Elastic Net Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.347\n",
      "\n",
      "Search finished after 00:14            \n",
      "Best pipeline: XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder\n",
      "Best pipeline Log Loss Binary: 0.236720\n",
      "14.713901042938232\n",
      "             Number of Features\n",
      "Boolean                       1\n",
      "Categorical                   6\n",
      "Numeric                       5\n",
      "\n",
      "Number of training examples: 2048\n",
      "Targets\n",
      "False    86.23%\n",
      "True     13.77%\n",
      "Name: fraud, dtype: object\n",
      "Using default limit of max_batches=1.\n",
      "\n",
      "Generating pipelines to search over...\n",
      "\n",
      "*****************************\n",
      "* Beginning pipeline search *\n",
      "*****************************\n",
      "\n",
      "Optimizing for Log Loss Binary. \n",
      "Lower score is better.\n",
      "\n",
      "Using DaskEngine to train and score pipelines.\n",
      "Searching up to 1 batches for a total of 9 pipelines. \n",
      "Allowed model families: xgboost, random_forest, extra_trees, decision_tree, lightgbm, linear_model, catboost\n",
      "\n",
      "Evaluating Baseline Pipeline: Mode Baseline Binary Classification Pipeline\n",
      "Mode Baseline Binary Classification Pipeline:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 4.756\n",
      "\n",
      "*****************************\n",
      "* Evaluating Batch Number 1 *\n",
      "*****************************\n",
      "\n",
      "CatBoost Classifier w/ Imputer + DateTime Featurization Component:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.534\n",
      "Decision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 1.032\n",
      "\tHigh coefficient of variation (cv >= 0.2) within cross validation scores.\n",
      "\tDecision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder may not perform as estimated on unseen data.\n",
      "XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.210\n",
      "LightGBM Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.283\n",
      "Elastic Net Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.416\n",
      "Extra Trees Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.363\n",
      "Logistic Regression Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.368\n",
      "Random Forest Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.267\n",
      "\n",
      "Search finished after 00:09            \n",
      "Best pipeline: XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder\n",
      "Best pipeline Log Loss Binary: 0.209978\n",
      "9.397616147994995\n",
      "             Number of Features\n",
      "Boolean                       1\n",
      "Categorical                   6\n",
      "Numeric                       5\n",
      "\n",
      "Number of training examples: 32768\n",
      "Targets\n",
      "False    84.90%\n",
      "True     15.10%\n",
      "Name: fraud, dtype: object\n",
      "Using default limit of max_batches=1.\n",
      "\n",
      "Generating pipelines to search over...\n",
      "\n",
      "*****************************\n",
      "* Beginning pipeline search *\n",
      "*****************************\n",
      "\n",
      "Optimizing for Log Loss Binary. \n",
      "Lower score is better.\n",
      "\n",
      "Using DaskEngine to train and score pipelines.\n",
      "Searching up to 1 batches for a total of 9 pipelines. \n",
      "Allowed model families: xgboost, random_forest, extra_trees, decision_tree, lightgbm, linear_model, catboost\n",
      "\n",
      "Evaluating Baseline Pipeline: Mode Baseline Binary Classification Pipeline\n",
      "Mode Baseline Binary Classification Pipeline:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 5.215\n",
      "\n",
      "*****************************\n",
      "* Evaluating Batch Number 1 *\n",
      "*****************************\n",
      "\n",
      "CatBoost Classifier w/ Imputer + DateTime Featurization Component:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.516\n",
      "Decision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.377\n",
      "\tHigh coefficient of variation (cv >= 0.2) within cross validation scores.\n",
      "\tDecision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder may not perform as estimated on unseen data.\n",
      "Extra Trees Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.370\n",
      "Random Forest Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.267\n",
      "XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.189\n",
      "Logistic Regression Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.330\n",
      "Elastic Net Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.433\n",
      "LightGBM Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.190\n",
      "\n",
      "Search finished after 00:16            \n",
      "Best pipeline: XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best pipeline Log Loss Binary: 0.188965\n",
      "16.908863067626953\n",
      "             Number of Features\n",
      "Boolean                       1\n",
      "Categorical                   6\n",
      "Numeric                       5\n",
      "\n",
      "Number of training examples: 99992\n",
      "Targets\n",
      "False    84.82%\n",
      "True     15.18%\n",
      "Name: fraud, dtype: object\n",
      "Using default limit of max_batches=1.\n",
      "\n",
      "Generating pipelines to search over...\n",
      "\n",
      "*****************************\n",
      "* Beginning pipeline search *\n",
      "*****************************\n",
      "\n",
      "Optimizing for Log Loss Binary. \n",
      "Lower score is better.\n",
      "\n",
      "Using DaskEngine to train and score pipelines.\n",
      "Searching up to 1 batches for a total of 9 pipelines. \n",
      "Allowed model families: xgboost, random_forest, extra_trees, decision_tree, lightgbm, linear_model, catboost\n",
      "\n",
      "Evaluating Baseline Pipeline: Mode Baseline Binary Classification Pipeline\n",
      "Mode Baseline Binary Classification Pipeline:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 5.243\n",
      "\n",
      "*****************************\n",
      "* Evaluating Batch Number 1 *\n",
      "*****************************\n",
      "\n",
      "CatBoost Classifier w/ Imputer + DateTime Featurization Component:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.514\n",
      "Decision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.435\n",
      "Extra Trees Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.373\n",
      "Random Forest Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.262\n",
      "Logistic Regression Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.332\n",
      "Elastic Net Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.434\n",
      "XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.192\n",
      "LightGBM Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.193\n",
      "\n",
      "Search finished after 00:33            \n",
      "Best pipeline: XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder\n",
      "Best pipeline Log Loss Binary: 0.191789\n",
      "33.59704399108887\n",
      "             Number of Features\n",
      "Boolean                       1\n",
      "Categorical                   6\n",
      "Numeric                       5\n",
      "\n",
      "Number of training examples: 99992\n",
      "Targets\n",
      "False    84.82%\n",
      "True     15.18%\n",
      "Name: fraud, dtype: object\n",
      "Using default limit of max_batches=1.\n",
      "\n",
      "Generating pipelines to search over...\n",
      "\n",
      "*****************************\n",
      "* Beginning pipeline search *\n",
      "*****************************\n",
      "\n",
      "Optimizing for Log Loss Binary. \n",
      "Lower score is better.\n",
      "\n",
      "Using DaskEngine to train and score pipelines.\n",
      "Searching up to 1 batches for a total of 9 pipelines. \n",
      "Allowed model families: xgboost, random_forest, extra_trees, decision_tree, lightgbm, linear_model, catboost\n",
      "\n",
      "Evaluating Baseline Pipeline: Mode Baseline Binary Classification Pipeline\n",
      "Mode Baseline Binary Classification Pipeline:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 5.243\n",
      "\n",
      "*****************************\n",
      "* Evaluating Batch Number 1 *\n",
      "*****************************\n",
      "\n",
      "CatBoost Classifier w/ Imputer + DateTime Featurization Component:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.514\n",
      "Decision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.435\n",
      "Extra Trees Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.373\n",
      "Random Forest Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.262\n",
      "Elastic Net Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.434\n",
      "Logistic Regression Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.332\n",
      "XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.192\n",
      "LightGBM Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.193\n",
      "\n",
      "Search finished after 00:38            \n",
      "Best pipeline: XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder\n",
      "Best pipeline Log Loss Binary: 0.191789\n",
      "38.087656021118164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/evalml_dev/lib/python3.9/site-packages/distributed/node.py:151: UserWarning:\n",
      "\n",
      "Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 51193 instead\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Number of Features\n",
      "Boolean                       1\n",
      "Categorical                   6\n",
      "Numeric                       5\n",
      "\n",
      "Number of training examples: 128\n",
      "Targets\n",
      "False    89.06%\n",
      "True     10.94%\n",
      "Name: fraud, dtype: object\n",
      "Using default limit of max_batches=1.\n",
      "\n",
      "Generating pipelines to search over...\n",
      "\n",
      "*****************************\n",
      "* Beginning pipeline search *\n",
      "*****************************\n",
      "\n",
      "Optimizing for Log Loss Binary. \n",
      "Lower score is better.\n",
      "\n",
      "Using DaskEngine to train and score pipelines.\n",
      "Searching up to 1 batches for a total of 9 pipelines. \n",
      "Allowed model families: xgboost, random_forest, extra_trees, decision_tree, lightgbm, linear_model, catboost\n",
      "\n",
      "Evaluating Baseline Pipeline: Mode Baseline Binary Classification Pipeline\n",
      "Mode Baseline Binary Classification Pipeline:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 3.774\n",
      "\n",
      "*****************************\n",
      "* Evaluating Batch Number 1 *\n",
      "*****************************\n",
      "\n",
      "CatBoost Classifier w/ Imputer + DateTime Featurization Component:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.594\n",
      "Decision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 3.994\n",
      "\tHigh coefficient of variation (cv >= 0.2) within cross validation scores.\n",
      "\tDecision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder may not perform as estimated on unseen data.\n",
      "LightGBM Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.339\n",
      "XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.237\n",
      "Extra Trees Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.365\n",
      "Random Forest Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.274\n",
      "Elastic Net Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.347\n",
      "Logistic Regression Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.700\n",
      "\n",
      "Search finished after 00:18            \n",
      "Best pipeline: XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder\n",
      "Best pipeline Log Loss Binary: 0.236720\n",
      "18.59212589263916\n",
      "             Number of Features\n",
      "Boolean                       1\n",
      "Categorical                   6\n",
      "Numeric                       5\n",
      "\n",
      "Number of training examples: 2048\n",
      "Targets\n",
      "False    86.23%\n",
      "True     13.77%\n",
      "Name: fraud, dtype: object\n",
      "Using default limit of max_batches=1.\n",
      "\n",
      "Generating pipelines to search over...\n",
      "\n",
      "*****************************\n",
      "* Beginning pipeline search *\n",
      "*****************************\n",
      "\n",
      "Optimizing for Log Loss Binary. \n",
      "Lower score is better.\n",
      "\n",
      "Using DaskEngine to train and score pipelines.\n",
      "Searching up to 1 batches for a total of 9 pipelines. \n",
      "Allowed model families: xgboost, random_forest, extra_trees, decision_tree, lightgbm, linear_model, catboost\n",
      "\n",
      "Evaluating Baseline Pipeline: Mode Baseline Binary Classification Pipeline\n",
      "Mode Baseline Binary Classification Pipeline:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 4.756\n",
      "\n",
      "*****************************\n",
      "* Evaluating Batch Number 1 *\n",
      "*****************************\n",
      "\n",
      "CatBoost Classifier w/ Imputer + DateTime Featurization Component:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.534\n",
      "Decision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 1.032\n",
      "\tHigh coefficient of variation (cv >= 0.2) within cross validation scores.\n",
      "\tDecision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder may not perform as estimated on unseen data.\n",
      "XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.210\n",
      "LightGBM Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.283\n",
      "Extra Trees Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.363\n",
      "Random Forest Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.267\n",
      "Elastic Net Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.416\n",
      "Logistic Regression Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.368\n",
      "\n",
      "Search finished after 00:12            \n",
      "Best pipeline: XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder\n",
      "Best pipeline Log Loss Binary: 0.209978\n",
      "12.585875988006592\n",
      "             Number of Features\n",
      "Boolean                       1\n",
      "Categorical                   6\n",
      "Numeric                       5\n",
      "\n",
      "Number of training examples: 32768\n",
      "Targets\n",
      "False    84.90%\n",
      "True     15.10%\n",
      "Name: fraud, dtype: object\n",
      "Using default limit of max_batches=1.\n",
      "\n",
      "Generating pipelines to search over...\n",
      "\n",
      "*****************************\n",
      "* Beginning pipeline search *\n",
      "*****************************\n",
      "\n",
      "Optimizing for Log Loss Binary. \n",
      "Lower score is better.\n",
      "\n",
      "Using DaskEngine to train and score pipelines.\n",
      "Searching up to 1 batches for a total of 9 pipelines. \n",
      "Allowed model families: xgboost, random_forest, extra_trees, decision_tree, lightgbm, linear_model, catboost\n",
      "\n",
      "Evaluating Baseline Pipeline: Mode Baseline Binary Classification Pipeline\n",
      "Mode Baseline Binary Classification Pipeline:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 5.215\n",
      "\n",
      "*****************************\n",
      "* Evaluating Batch Number 1 *\n",
      "*****************************\n",
      "\n",
      "CatBoost Classifier w/ Imputer + DateTime Featurization Component:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.516\n",
      "Decision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.377\n",
      "\tHigh coefficient of variation (cv >= 0.2) within cross validation scores.\n",
      "\tDecision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder may not perform as estimated on unseen data.\n",
      "Extra Trees Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.370\n",
      "Random Forest Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.267\n",
      "Elastic Net Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.433\n",
      "Logistic Regression Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.330\n",
      "XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.189\n",
      "LightGBM Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.190\n",
      "\n",
      "Search finished after 00:17            \n",
      "Best pipeline: XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best pipeline Log Loss Binary: 0.188965\n",
      "17.011240005493164\n",
      "             Number of Features\n",
      "Boolean                       1\n",
      "Categorical                   6\n",
      "Numeric                       5\n",
      "\n",
      "Number of training examples: 99992\n",
      "Targets\n",
      "False    84.82%\n",
      "True     15.18%\n",
      "Name: fraud, dtype: object\n",
      "Using default limit of max_batches=1.\n",
      "\n",
      "Generating pipelines to search over...\n",
      "\n",
      "*****************************\n",
      "* Beginning pipeline search *\n",
      "*****************************\n",
      "\n",
      "Optimizing for Log Loss Binary. \n",
      "Lower score is better.\n",
      "\n",
      "Using DaskEngine to train and score pipelines.\n",
      "Searching up to 1 batches for a total of 9 pipelines. \n",
      "Allowed model families: xgboost, random_forest, extra_trees, decision_tree, lightgbm, linear_model, catboost\n",
      "\n",
      "Evaluating Baseline Pipeline: Mode Baseline Binary Classification Pipeline\n",
      "Mode Baseline Binary Classification Pipeline:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 5.243\n",
      "\n",
      "*****************************\n",
      "* Evaluating Batch Number 1 *\n",
      "*****************************\n",
      "\n",
      "CatBoost Classifier w/ Imputer + DateTime Featurization Component:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.514\n",
      "Decision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.435\n",
      "Extra Trees Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.373\n",
      "Random Forest Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.262\n",
      "Logistic Regression Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.332\n",
      "Elastic Net Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.434\n",
      "XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.192\n",
      "LightGBM Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.193\n",
      "\n",
      "Search finished after 00:36            \n",
      "Best pipeline: XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder\n",
      "Best pipeline Log Loss Binary: 0.191789\n",
      "36.40000677108765\n",
      "             Number of Features\n",
      "Boolean                       1\n",
      "Categorical                   6\n",
      "Numeric                       5\n",
      "\n",
      "Number of training examples: 99992\n",
      "Targets\n",
      "False    84.82%\n",
      "True     15.18%\n",
      "Name: fraud, dtype: object\n",
      "Using default limit of max_batches=1.\n",
      "\n",
      "Generating pipelines to search over...\n",
      "\n",
      "*****************************\n",
      "* Beginning pipeline search *\n",
      "*****************************\n",
      "\n",
      "Optimizing for Log Loss Binary. \n",
      "Lower score is better.\n",
      "\n",
      "Using DaskEngine to train and score pipelines.\n",
      "Searching up to 1 batches for a total of 9 pipelines. \n",
      "Allowed model families: xgboost, random_forest, extra_trees, decision_tree, lightgbm, linear_model, catboost\n",
      "\n",
      "Evaluating Baseline Pipeline: Mode Baseline Binary Classification Pipeline\n",
      "Mode Baseline Binary Classification Pipeline:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 5.243\n",
      "\n",
      "*****************************\n",
      "* Evaluating Batch Number 1 *\n",
      "*****************************\n",
      "\n",
      "CatBoost Classifier w/ Imputer + DateTime Featurization Component:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.514\n",
      "Decision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.435\n",
      "Random Forest Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.262\n",
      "Extra Trees Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.373\n",
      "Logistic Regression Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.332\n",
      "Elastic Net Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.434\n",
      "XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.192\n",
      "LightGBM Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.193\n",
      "\n",
      "Search finished after 00:34            \n",
      "Best pipeline: XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder\n",
      "Best pipeline Log Loss Binary: 0.191789\n",
      "34.12483596801758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/evalml_dev/lib/python3.9/site-packages/distributed/node.py:151: UserWarning:\n",
      "\n",
      "Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 51250 instead\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Number of Features\n",
      "Boolean                       1\n",
      "Categorical                   6\n",
      "Numeric                       5\n",
      "\n",
      "Number of training examples: 128\n",
      "Targets\n",
      "False    89.06%\n",
      "True     10.94%\n",
      "Name: fraud, dtype: object\n",
      "Using default limit of max_batches=1.\n",
      "\n",
      "Generating pipelines to search over...\n",
      "\n",
      "*****************************\n",
      "* Beginning pipeline search *\n",
      "*****************************\n",
      "\n",
      "Optimizing for Log Loss Binary. \n",
      "Lower score is better.\n",
      "\n",
      "Using DaskEngine to train and score pipelines.\n",
      "Searching up to 1 batches for a total of 9 pipelines. \n",
      "Allowed model families: xgboost, random_forest, extra_trees, decision_tree, lightgbm, linear_model, catboost\n",
      "\n",
      "Evaluating Baseline Pipeline: Mode Baseline Binary Classification Pipeline\n",
      "Mode Baseline Binary Classification Pipeline:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 3.774\n",
      "\n",
      "*****************************\n",
      "* Evaluating Batch Number 1 *\n",
      "*****************************\n",
      "\n",
      "Decision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 3.994\n",
      "\tHigh coefficient of variation (cv >= 0.2) within cross validation scores.\n",
      "\tDecision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder may not perform as estimated on unseen data.\n",
      "CatBoost Classifier w/ Imputer + DateTime Featurization Component:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.594\n",
      "LightGBM Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.339\n",
      "XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.237\n",
      "Extra Trees Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.365\n",
      "Random Forest Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.274\n",
      "Elastic Net Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.347\n",
      "Logistic Regression Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.700\n",
      "\n",
      "Search finished after 00:15            \n",
      "Best pipeline: XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder\n",
      "Best pipeline Log Loss Binary: 0.236720\n",
      "15.31468415260315\n",
      "             Number of Features\n",
      "Boolean                       1\n",
      "Categorical                   6\n",
      "Numeric                       5\n",
      "\n",
      "Number of training examples: 2048\n",
      "Targets\n",
      "False    86.23%\n",
      "True     13.77%\n",
      "Name: fraud, dtype: object\n",
      "Using default limit of max_batches=1.\n",
      "\n",
      "Generating pipelines to search over...\n",
      "\n",
      "*****************************\n",
      "* Beginning pipeline search *\n",
      "*****************************\n",
      "\n",
      "Optimizing for Log Loss Binary. \n",
      "Lower score is better.\n",
      "\n",
      "Using DaskEngine to train and score pipelines.\n",
      "Searching up to 1 batches for a total of 9 pipelines. \n",
      "Allowed model families: xgboost, random_forest, extra_trees, decision_tree, lightgbm, linear_model, catboost\n",
      "\n",
      "Evaluating Baseline Pipeline: Mode Baseline Binary Classification Pipeline\n",
      "Mode Baseline Binary Classification Pipeline:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 4.756\n",
      "\n",
      "*****************************\n",
      "* Evaluating Batch Number 1 *\n",
      "*****************************\n",
      "\n",
      "CatBoost Classifier w/ Imputer + DateTime Featurization Component:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.534\n",
      "Decision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 1.032\n",
      "\tHigh coefficient of variation (cv >= 0.2) within cross validation scores.\n",
      "\tDecision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder may not perform as estimated on unseen data.\n",
      "XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.210\n",
      "LightGBM Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.283\n",
      "Random Forest Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.267\n",
      "Extra Trees Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.363\n",
      "Elastic Net Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.416\n",
      "Logistic Regression Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.368\n",
      "\n",
      "Search finished after 00:10            \n",
      "Best pipeline: XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder\n",
      "Best pipeline Log Loss Binary: 0.209978\n",
      "10.250228881835938\n",
      "             Number of Features\n",
      "Boolean                       1\n",
      "Categorical                   6\n",
      "Numeric                       5\n",
      "\n",
      "Number of training examples: 32768\n",
      "Targets\n",
      "False    84.90%\n",
      "True     15.10%\n",
      "Name: fraud, dtype: object\n",
      "Using default limit of max_batches=1.\n",
      "\n",
      "Generating pipelines to search over...\n",
      "\n",
      "*****************************\n",
      "* Beginning pipeline search *\n",
      "*****************************\n",
      "\n",
      "Optimizing for Log Loss Binary. \n",
      "Lower score is better.\n",
      "\n",
      "Using DaskEngine to train and score pipelines.\n",
      "Searching up to 1 batches for a total of 9 pipelines. \n",
      "Allowed model families: xgboost, random_forest, extra_trees, decision_tree, lightgbm, linear_model, catboost\n",
      "\n",
      "Evaluating Baseline Pipeline: Mode Baseline Binary Classification Pipeline\n",
      "Mode Baseline Binary Classification Pipeline:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 5.215\n",
      "\n",
      "*****************************\n",
      "* Evaluating Batch Number 1 *\n",
      "*****************************\n",
      "\n",
      "CatBoost Classifier w/ Imputer + DateTime Featurization Component:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.516\n",
      "Decision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.377\n",
      "\tHigh coefficient of variation (cv >= 0.2) within cross validation scores.\n",
      "\tDecision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder may not perform as estimated on unseen data.\n",
      "Random Forest Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.267\n",
      "Extra Trees Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.370\n",
      "XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.189\n",
      "Elastic Net Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.433\n",
      "LightGBM Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.190\n",
      "Logistic Regression Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.330\n",
      "\n",
      "Search finished after 00:17            \n",
      "Best pipeline: XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best pipeline Log Loss Binary: 0.188965\n",
      "17.34153699874878\n",
      "             Number of Features\n",
      "Boolean                       1\n",
      "Categorical                   6\n",
      "Numeric                       5\n",
      "\n",
      "Number of training examples: 99992\n",
      "Targets\n",
      "False    84.82%\n",
      "True     15.18%\n",
      "Name: fraud, dtype: object\n",
      "Using default limit of max_batches=1.\n",
      "\n",
      "Generating pipelines to search over...\n",
      "\n",
      "*****************************\n",
      "* Beginning pipeline search *\n",
      "*****************************\n",
      "\n",
      "Optimizing for Log Loss Binary. \n",
      "Lower score is better.\n",
      "\n",
      "Using DaskEngine to train and score pipelines.\n",
      "Searching up to 1 batches for a total of 9 pipelines. \n",
      "Allowed model families: xgboost, random_forest, extra_trees, decision_tree, lightgbm, linear_model, catboost\n",
      "\n",
      "Evaluating Baseline Pipeline: Mode Baseline Binary Classification Pipeline\n",
      "Mode Baseline Binary Classification Pipeline:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 5.243\n",
      "\n",
      "*****************************\n",
      "* Evaluating Batch Number 1 *\n",
      "*****************************\n",
      "\n",
      "CatBoost Classifier w/ Imputer + DateTime Featurization Component:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.514\n",
      "Decision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.435\n",
      "Extra Trees Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.373\n",
      "Random Forest Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.262\n",
      "Logistic Regression Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.332\n",
      "Elastic Net Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.434\n",
      "XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.192\n",
      "LightGBM Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.193\n",
      "\n",
      "Search finished after 00:36            \n",
      "Best pipeline: XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder\n",
      "Best pipeline Log Loss Binary: 0.191789\n",
      "36.74541115760803\n",
      "             Number of Features\n",
      "Boolean                       1\n",
      "Categorical                   6\n",
      "Numeric                       5\n",
      "\n",
      "Number of training examples: 99992\n",
      "Targets\n",
      "False    84.82%\n",
      "True     15.18%\n",
      "Name: fraud, dtype: object\n",
      "Using default limit of max_batches=1.\n",
      "\n",
      "Generating pipelines to search over...\n",
      "\n",
      "*****************************\n",
      "* Beginning pipeline search *\n",
      "*****************************\n",
      "\n",
      "Optimizing for Log Loss Binary. \n",
      "Lower score is better.\n",
      "\n",
      "Using DaskEngine to train and score pipelines.\n",
      "Searching up to 1 batches for a total of 9 pipelines. \n",
      "Allowed model families: xgboost, random_forest, extra_trees, decision_tree, lightgbm, linear_model, catboost\n",
      "\n",
      "Evaluating Baseline Pipeline: Mode Baseline Binary Classification Pipeline\n",
      "Mode Baseline Binary Classification Pipeline:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 5.243\n",
      "\n",
      "*****************************\n",
      "* Evaluating Batch Number 1 *\n",
      "*****************************\n",
      "\n",
      "CatBoost Classifier w/ Imputer + DateTime Featurization Component:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.514\n",
      "Decision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.435\n",
      "Extra Trees Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.373\n",
      "Random Forest Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.262\n",
      "Elastic Net Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.434\n",
      "Logistic Regression Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.332\n",
      "XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.192\n",
      "LightGBM Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.193\n",
      "\n",
      "Search finished after 00:35            \n",
      "Best pipeline: XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder\n",
      "Best pipeline Log Loss Binary: 0.191789\n",
      "35.65751123428345\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "\n",
    "\n",
    "for engine_cfg in engine_cfgs:\n",
    "    engine = build_engine(engine_cfg)\n",
    "    for n_rows in [2**7, 2**11, 2**15, 2**19, 2**24]:\n",
    "        X, y = load_fraud(n_rows=n_rows, return_pandas=True)\n",
    "        automl = AutoMLSearch(X,y, problem_type=\"binary\", engine=engine)\n",
    "        automl.search(show_iteration_plot=False)\n",
    "        print(automl.search_duration)\n",
    "    \n",
    "        res.append((engine_cfg[\"type\"], \n",
    "                    engine_cfg[\"n_workers\"],\n",
    "                    n_rows, automl.search_duration, \n",
    "                    max(automl.full_rankings[\"validation_score\"])))\n",
    "    del engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128, 2048, 32768, 524288, 16777216]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[2**7, 2**11, 2**15, 2**19, 2**24]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sequential', 1, 128, 19.818434715270996, 4.016136790105894),\n",
       " ('sequential', 1, 2048, 19.826002836227417, 4.753506560939392),\n",
       " ('sequential', 1, 32768, 38.102290868759155, 5.214175801080996),\n",
       " ('sequential', 1, 524288, 74.45213007926941, 5.243353291477845),\n",
       " ('sequential', 1, 16777216, 80.4342348575592, 5.243353291477845),\n",
       " ('dask', 1, 128, 17.34480905532837, 4.016136790105894),\n",
       " ('dask', 1, 2048, 10.225600957870483, 4.753506560939392),\n",
       " ('dask', 1, 32768, 22.06515097618103, 5.214175801080996),\n",
       " ('dask', 1, 524288, 40.02791094779968, 5.243353291477845),\n",
       " ('dask', 1, 16777216, 32.725412130355835, 5.243353291477845),\n",
       " ('dask', 2, 128, 14.713901042938232, 4.016136790105894),\n",
       " ('dask', 2, 2048, 9.397616147994995, 4.753506560939392),\n",
       " ('dask', 2, 32768, 16.908863067626953, 5.214175801080996),\n",
       " ('dask', 2, 524288, 33.59704399108887, 5.243353291477845),\n",
       " ('dask', 2, 16777216, 38.087656021118164, 5.243353291477845),\n",
       " ('dask', 4, 128, 18.59212589263916, 4.016136790105894),\n",
       " ('dask', 4, 2048, 12.585875988006592, 4.753506560939392),\n",
       " ('dask', 4, 32768, 17.011240005493164, 5.214175801080996),\n",
       " ('dask', 4, 524288, 36.40000677108765, 5.243353291477845),\n",
       " ('dask', 4, 16777216, 34.12483596801758, 5.243353291477845),\n",
       " ('dask', 8, 128, 15.31468415260315, 4.016136790105894),\n",
       " ('dask', 8, 2048, 10.250228881835938, 4.753506560939392),\n",
       " ('dask', 8, 32768, 17.34153699874878, 5.214175801080996),\n",
       " ('dask', 8, 524288, 36.74541115760803, 5.243353291477845),\n",
       " ('dask', 8, 16777216, 35.65751123428345, 5.243353291477845)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-7b0489bbc7e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpar2_aml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpar4_aml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mpar8_aml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-7b0489bbc7e8>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpar2_aml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpar4_aml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mpar8_aml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_rows = [x[0] for x in res]\n",
    "seq_aml = [x[1] for x in res]\n",
    "par1_aml = [x[2] for x in res]\n",
    "par2_aml = [x[3] for x in res]\n",
    "par4_aml = [x[4] for x in res]\n",
    "par8_aml = [x[5] for x in res]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title(\"Fraud Dataset: Search time vs. n_rows\")\n",
    "plt.scatter(n_rows, seq_aml, label=\"Sequential Engine\")\n",
    "plt.scatter(n_rows, par1_aml, label=\"Parallel Engine, n_workers=1\")\n",
    "plt.scatter(n_rows, par2_aml, label=\"Parallel Engine, n_workers=2\")\n",
    "plt.scatter(n_rows, par4_aml, label=\"Parallel Engine, n_workers=4\")\n",
    "plt.scatter(n_rows, par8_aml, label=\"Parallel Engine, n_workers=8\")\n",
    "plt.xlabel(\"n_rows\")\n",
    "plt.ylabel(\"Fit time (s)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_scores = [x[6] for x in res]\n",
    "par1_scores = [x[7] for x in res]\n",
    "par2_scores = [x[8] for x in res]\n",
    "par4_scores = [x[9] for x in res]\n",
    "par8_scores = [x[10] for x in res]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par1_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par8_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
